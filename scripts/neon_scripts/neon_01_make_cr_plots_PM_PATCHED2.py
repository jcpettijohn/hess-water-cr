#!/usr/bin/env python3
"""
NEON proof-of-concept: compute daily E, E_pa, and E_p0 (two constructions) from NEON 30-min tables.

This script is intentionally "physics-consistent" with the manuscript's Penman–Monteith form:

    λE = [Δ (Rn - G) + ρ c_p VPD / r_a] / [Δ + γ (1 + r_s/r_a)]

and defines the saturated-patch diagnostic E_pa by setting r_s = 0.

Key practical points for NEON tower data:
- NEON DP1 "sensor" tables (radiation, wind, RH, etc.) often contain multiple sensor positions per timestamp
  (horizontalPosition/verticalPosition). If you merge these tables without collapsing to 1 row per time,
  you'll silently duplicate rows and inflate daily totals. This script collapses by time and (optionally)
  selects a specific measurement height.
- Compute the PET constructions at the native 30-min resolution and THEN aggregate to daily totals.
  Using daily-mean u and daily-mean RH/T tends to underestimate aerodynamic demand (Jensen's inequality).

Outputs:
- daily CSV with E, Epa, Ep0_eq (α=1), Ep0_PT (α=α_PT), plus nondimensional x and y.
- a small QC summary printed to stdout.

Expected input directory layout (from your existing workflow):
  <root>/<SITE>_<start>_<end>/tables/
    eddy_fluxH2O_dp04_30min.parquet
    rad_DP1_00023_001_30min.parquet
    g_DP1_00040_001_30min.parquet
    rh_DP1_00098_001_30min.parquet
    wind_DP1_00001_001_30min.parquet
    pres_DP1_00004_001_30min.parquet

If your stems differ, use --stem-* args.

Author: (generated by ChatGPT)
"""
from __future__ import annotations

import argparse
import math
import sys
from pathlib import Path
from typing import Iterable, Optional, Sequence, Tuple

import numpy as np
import pandas as pd


# -----------------------------
# Constants / thermo utilities
# -----------------------------
KAPPA = 0.4  # von Karman
CP_AIR = 1004.0  # J kg-1 K-1
R_D = 287.05  # J kg-1 K-1 (dry air)
EPSILON = 0.622  # ratio molecular weights (water/dry air)

# default "canopy geometry" scalings for roughness/zero-plane displacement
# (common rough approximations for tall vegetation; OK for proof-of-concept)
DEFAULT_D_OVER_H = 0.67
DEFAULT_Z0M_OVER_H = 0.123
DEFAULT_Z0H_OVER_Z0M = 0.1


def sat_vp_kpa(T_C: np.ndarray) -> np.ndarray:
    """Saturation vapor pressure (kPa); FAO-56 style."""
    return 0.6108 * np.exp((17.27 * T_C) / (T_C + 237.3))


def slope_sat_vp_curve_kpa_per_C(T_C: np.ndarray) -> np.ndarray:
    """Slope of saturation vapor pressure curve (kPa / °C)."""
    es = sat_vp_kpa(T_C)
    return 4098.0 * es / np.square(T_C + 237.3)


def latent_heat_vaporization_J_per_kg(T_C: np.ndarray) -> np.ndarray:
    """Latent heat of vaporization (J/kg)."""
    # widely used linear approximation
    return 2.501e6 - 2370.0 * T_C


def psychrometric_constant_kpa_per_C(p_kpa: np.ndarray, lam_J_kg: np.ndarray) -> np.ndarray:
    """Psychrometric constant (kPa / °C)."""
    # gamma = c_p * p / (epsilon * lambda)
    return (CP_AIR * p_kpa) / (EPSILON * lam_J_kg)



def pressure_to_kpa(p: np.ndarray) -> np.ndarray:
    """Convert a pressure array to kPa using simple magnitude heuristics.

    NEON pressure tables may appear in:
      - kPa (typical values ~90--105)
      - hPa (typical values ~900--1050)
      - Pa  (typical values ~90000--105000)

    This helper makes the script robust across those conventions.
    """
    p = np.asarray(p, dtype=float)
    med = np.nanmedian(p)
    if not np.isfinite(med):
        return p * np.nan
    if med > 2000.0:  # Pa
        return p / 1000.0
    if med > 200.0:  # hPa
        return p / 10.0
    return p  # already kPa

def vpd_kpa_from_rh(T_C: np.ndarray, RH_frac: np.ndarray) -> np.ndarray:
    """Vapor pressure deficit (kPa) from T and RH (fraction 0-1)."""
    es = sat_vp_kpa(T_C)
    ea = np.clip(RH_frac, 0.0, 1.0) * es
    return np.maximum(0.0, es - ea)


def specific_humidity_from_rh(T_C: np.ndarray, RH_frac: np.ndarray, p_kpa: np.ndarray) -> np.ndarray:
    """
    Specific humidity q (kg/kg) from RH, T, and pressure.
    """
    es = sat_vp_kpa(T_C)  # kPa
    ea = np.clip(RH_frac, 0.0, 1.0) * es  # kPa
    # mixing ratio w = epsilon * ea / (p - ea)
    w = EPSILON * ea / np.maximum(p_kpa - ea, 1e-6)
    q = w / (1.0 + w)
    return np.clip(q, 0.0, 0.2)  # sanity


def air_density_kg_m3(T_C: np.ndarray, p_kpa: np.ndarray, q: np.ndarray) -> np.ndarray:
    """
    Moist air density (kg/m3), ideal gas with virtual temperature correction.
    """
    T_K = T_C + 273.15
    p_pa = p_kpa * 1000.0
    Tv = T_K * (1.0 + 0.61 * q)
    return p_pa / (R_D * np.maximum(Tv, 1.0))


def ra_neutral_s_per_m(
    u_ms: np.ndarray,
    z_m: float,
    h_canopy_m: float,
    d_over_h: float = DEFAULT_D_OVER_H,
    z0m_over_h: float = DEFAULT_Z0M_OVER_H,
    z0h_over_z0m: float = DEFAULT_Z0H_OVER_Z0M,
    u_floor: float = 0.2,
) -> np.ndarray:
    """
    Neutral aerodynamic resistance (s/m) using a simple log-law bulk form:

        r_a = ln((z-d)/z0m) * ln((z-d)/z0h) / (k^2 u)

    This is crude (no stability correction), but is adequate for a proof-of-concept and
    far closer to the manuscript's model form than FAO's baked-in coefficients.

    z_m: reference measurement height (m), e.g. flux/wind measurement height.
    h_canopy_m: canopy height (m). For HARV, a plausible order is 15–25 m.
    """
    u = np.maximum(u_ms, u_floor)
    h = max(float(h_canopy_m), 0.1)
    d = d_over_h * h
    z0m = z0m_over_h * h
    z0h = z0h_over_z0m * z0m

    # enforce geometric sanity
    z_eff = max(float(z_m) - d, 0.1 * z0m)
    z_eff = max(z_eff, 1e-3)

    ln_m = np.log(np.maximum(z_eff / z0m, 1.0001))
    ln_h = np.log(np.maximum(z_eff / z0h, 1.0001))
    return (ln_m * ln_h) / (KAPPA**2 * u)


def penman_monteith_LE_Wm2(
    RnG_Wm2: np.ndarray,
    T_C: np.ndarray,
    RH_frac: np.ndarray,
    p_kpa: np.ndarray,
    u_ms: np.ndarray,
    r_a_s_m: np.ndarray,
    r_s_s_m: float,
    clip_negative: bool = True,
) -> np.ndarray:
    """
    Manuscript-consistent Penman–Monteith latent heat flux (W/m2).

    λE = [Δ RnG + ρ c_p VPD / r_a] / [Δ + γ(1 + r_s/r_a)]
    """
    # sanitize inputs
    RnG = np.array(RnG_Wm2, dtype=float)
    T = np.array(T_C, dtype=float)
    RH = np.array(RH_frac, dtype=float)
    p = np.array(p_kpa, dtype=float)
    u = np.array(u_ms, dtype=float)
    ra = np.array(r_a_s_m, dtype=float)

    if clip_negative:
        RnG = np.maximum(0.0, RnG)

    lam = latent_heat_vaporization_J_per_kg(T)
    Delta = slope_sat_vp_curve_kpa_per_C(T)
    gamma = psychrometric_constant_kpa_per_C(p, lam)
    VPD = vpd_kpa_from_rh(T, RH)
    q = specific_humidity_from_rh(T, RH, p)
    rho = air_density_kg_m3(T, p, q)

    num = Delta * RnG + rho * CP_AIR * VPD / np.maximum(ra, 1e-6)
    den = Delta + gamma * (1.0 + max(float(r_s_s_m), 0.0) / np.maximum(ra, 1e-6))

    LE = num / np.maximum(den, 1e-12)
    if clip_negative:
        LE = np.maximum(0.0, LE)
    return LE


def priestley_taylor_LE_Wm2(
    RnG_Wm2: np.ndarray,
    T_C: np.ndarray,
    p_kpa: np.ndarray,
    alpha: float = 1.26,
    clip_negative: bool = True,
) -> np.ndarray:
    """
    Priestley–Taylor wet benchmark latent heat flux (W/m2):

        λE_PT = α * (Δ/(Δ+γ)) * (Rn - G)

    Note: strict CR definition consistency would use a wet-reference air state. Here we
    intentionally compute both α=1 and α=1.26 from the *same observed air state* to
    demonstrate definition sensitivity (as in the manuscript Tier-1B experiments).
    """
    RnG = np.array(RnG_Wm2, dtype=float)
    T = np.array(T_C, dtype=float)
    p = np.array(p_kpa, dtype=float)

    if clip_negative:
        RnG = np.maximum(0.0, RnG)

    lam = latent_heat_vaporization_J_per_kg(T)
    Delta = slope_sat_vp_curve_kpa_per_C(T)
    gamma = psychrometric_constant_kpa_per_C(p, lam)

    LE = float(alpha) * (Delta / np.maximum(Delta + gamma, 1e-12)) * RnG
    if clip_negative:
        LE = np.maximum(0.0, LE)
    return LE


# -----------------------------
# I/O helpers
# -----------------------------
def read_table_anycase(stem_or_path: Path) -> pd.DataFrame:
    """
    Read a parquet or csv table, allowing for case-insensitive stem matches on case-sensitive FS.

    If `stem_or_path` has a suffix, use it directly.
    Otherwise try stem.parquet then stem.csv.
    If not found, perform a case-insensitive glob in the directory for matching stem.
    """
    p = Path(stem_or_path)
    if p.suffix.lower() in {".parquet", ".pq"} and p.exists():
        return pd.read_parquet(p)
    if p.suffix.lower() == ".csv" and p.exists():
        return pd.read_csv(p)

    # try direct extensions
    for ext, reader in [(".parquet", pd.read_parquet), (".csv", pd.read_csv)]:
        cand = p.with_suffix(ext)
        if cand.exists():
            return reader(cand)

    # case-insensitive search
    parent = p.parent
    stem = p.name.lower()
    for ext in [".parquet", ".csv"]:
        matches = [m for m in parent.glob(f"*{ext}") if m.stem.lower() == stem]
        if matches:
            m = matches[0]
            return pd.read_parquet(m) if ext == ".parquet" else pd.read_csv(m)

    raise FileNotFoundError(f"Could not find table for stem/path: {p}")


def find_col(df: pd.DataFrame, candidates: Sequence[str], contains: Sequence[str] = ()) -> str:
    """
    Robust column finder:
    1) exact match by candidate names (case-insensitive)
    2) contains match by substring list (case-insensitive)
    """
    cols_lower = {c.lower(): c for c in df.columns}
    for c in candidates:
        if c.lower() in cols_lower:
            return cols_lower[c.lower()]
    for sub in contains:
        sub_l = sub.lower()
        for c in df.columns:
            if sub_l in c.lower():
                return c
    raise KeyError(
        f"Could not find column. Tried={list(candidates)} contains={list(contains)}. "
        f"Available={list(df.columns)[:40]}..."
    )


def find_time_col(df: pd.DataFrame) -> str:
    """
    Return the name of the column that carries timestamps.

    NEON tables are not fully consistent:
      - DP1 instrument / observational tables typically use startDateTime / endDateTime
      - NEON surface-atmosphere exchange (SAE; eddy covariance) tables often use timeBgn / timeEnd

    We prefer the *interval-begin* timestamp (startDateTime / timeBgn) so that different products
    align cleanly when merged.
    """
    # common exact names (ordered by preference)
    for c in [
        # generic
        "time",
        # DP1 common
        "startDateTime",
        "startDateTime_UTC",
        "startDateTimeLocal",
        "dateTime",
        # SAE / flux common
        "timeBgn",
        "timeBegin",
        "timeStart",
        # fallbacks
        "endDateTime",
        "timeEnd",
        "timeStop",
    ]:
        if c in df.columns:
            return c

    # looser fallback: pick a column containing "time" that is mostly parseable as datetime
    time_like = [c for c in df.columns if "time" in c.lower()]
    best = None
    best_frac = 0.0
    n = len(df)
    if n == 0:
        raise KeyError("Empty dataframe; cannot identify time column.")
    for c in time_like:
        parsed = pd.to_datetime(df[c], errors="coerce", utc=True)
        frac = float(parsed.notna().mean())
        if frac > best_frac:
            best = c
            best_frac = frac
    if best is not None and best_frac > 0.5:
        return best

    raise KeyError(f"Could not find time column. Columns={list(df.columns)[:40]}...")


def qf_colname(df: pd.DataFrame) -> Optional[str]:
    """Return a likely quality flag column name, if present."""
    for c in ["qfFinl", "finalQF", "finalQFMean", "qfFinal", "finalQFfinal"]:
        if c in df.columns:
            return c
    # some NEON DP1 tables use "finalQF" exactly (as in your screenshot)
    for c in df.columns:
        if c.lower() in {"finalqf", "qffinal", "qffinl"}:
            return c
    return None


def collapse_by_time(
    df: pd.DataFrame,
    time_col: str,
    value_cols: Sequence[str],
    *,
    qf: Optional[str] = None,
    qf_good: int = 0,
    vertical_select: Optional[str] = None,
    vertical_col: str = "verticalPosition",
) -> Tuple[pd.DataFrame, Optional[float]]:
    """
    Collapse NEON tables to one row per timestamp.

    vertical_select:
        None -> ignore verticalPosition (average across all)
        "max" -> keep only rows at the maximum verticalPosition
        "min" -> keep only rows at the minimum verticalPosition
        "<number>" -> keep only rows with verticalPosition closest to that number (in meters)

    Returns:
        collapsed dataframe with columns: time + value_cols
        chosen verticalPosition (or None if not used)
    """
    d = df.copy()

    # parse time
    d[time_col] = pd.to_datetime(d[time_col], errors="coerce", utc=True)
    d = d.dropna(subset=[time_col])

    # quality filtering
    if qf is not None and qf in d.columns:
        d = d.loc[d[qf] == qf_good].copy()

    chosen_z = None
    if vertical_select is not None and vertical_col in d.columns:
        zvals = pd.to_numeric(d[vertical_col], errors="coerce")
        d = d.assign(_z=zvals)
        d = d.dropna(subset=["_z"])
        if len(d) == 0:
            return d[[time_col] + list(value_cols)], None

        if vertical_select.lower() == "max":
            chosen_z = float(d["_z"].max())
        elif vertical_select.lower() == "min":
            chosen_z = float(d["_z"].min())
        else:
            try:
                target = float(vertical_select)
                chosen_z = float(d.loc[(d["_z"] - target).abs().idxmin(), "_z"])
            except Exception:
                chosen_z = float(d["_z"].max())

        d = d.loc[np.isclose(d["_z"], chosen_z)].copy()
        d = d.drop(columns=["_z"])

    # keep only required columns
    keep = [time_col] + [c for c in value_cols if c in d.columns]
    d = d[keep]

    # collapse to unique timestamps (mean across sensor positions)
    if d[time_col].duplicated().any():
        d = d.groupby(time_col, as_index=False).mean(numeric_only=True)

    return d, chosen_z


def expected_intervals_per_day(dt_minutes: int) -> int:
    return int(round(24 * 60 / dt_minutes))


# -----------------------------
# Main processing
# -----------------------------
def build_30min_dataframe(
    tables_dir: Path,
    *,
    stem_flux: str,
    stem_rad: str,
    stem_g: str,
    stem_rh: str,
    stem_wind: str,
    stem_pres: str,
    dt_minutes: int,
    select_height: str = "max",
) -> Tuple[pd.DataFrame, dict]:
    """
    Read tables, select/average sensor positions, and merge into a single 30-min dataframe with
    one row per timestamp.

    Returns merged dataframe and metadata dict (chosen heights etc).
    """
    meta = {}

    # ---- Eddy flux (latent heat flux) ----
    flux = read_table_anycase(tables_dir / stem_flux)
    t_flux = find_time_col(flux)
    le_col = find_col(flux, ["fluxH2o", "fluxH2O", "latentHeatFlux", "LE"], contains=["fluxh2o"])
    qf_flux = qf_colname(flux)
    flux_use, _ = collapse_by_time(flux, t_flux, [le_col], qf=qf_flux, vertical_select=None)
    flux_use = flux_use.rename(columns={t_flux: "time", le_col: "LE_obs_Wm2"})
    meta["flux_col"] = le_col
    meta["flux_qf"] = qf_flux

    # ---- Radiation components ----
    rad = read_table_anycase(tables_dir / stem_rad)
    t_rad = find_time_col(rad)
    in_sw = find_col(rad, ["inSWMean"], contains=["insw"])
    out_sw = find_col(rad, ["outSWMean"], contains=["outsw"])
    in_lw = find_col(rad, ["inLWMean"], contains=["inlw"])
    out_lw = find_col(rad, ["outLWMean"], contains=["outlw"])
    qf_rad = qf_colname(rad)
    rad_use, _ = collapse_by_time(
        rad,
        t_rad,
        [in_sw, out_sw, in_lw, out_lw],
        qf=qf_rad,
        vertical_select=None,  # net radiation; average across positions
    )
    rad_use = rad_use.rename(columns={t_rad: "time", in_sw: "inSW", out_sw: "outSW", in_lw: "inLW", out_lw: "outLW"})
    meta["rad_qf"] = qf_rad

    # ---- Soil heat flux ----
    gdf = read_table_anycase(tables_dir / stem_g)
    t_g = find_time_col(gdf)
    g_col = find_col(gdf, ["soilHeatFluxMean", "SHFMean"], contains=["shf", "soilheatflux", "heatflux"])
    qf_g = qf_colname(gdf)
    g_use, _ = collapse_by_time(gdf, t_g, [g_col], qf=qf_g, vertical_select=None)
    g_use = g_use.rename(columns={t_g: "time", g_col: "G_Wm2"})
    meta["g_col"] = g_col
    meta["g_qf"] = qf_g

    # ---- Relative humidity + air temperature ----
    rhdf = read_table_anycase(tables_dir / stem_rh)
    t_rh = find_time_col(rhdf)
    # NEON often stores RH and air temperature in the same table DP1.00098
    rh_col = find_col(rhdf, ["RHMean", "RHMean_30min", "rhMean"], contains=["rhmean", "relativehumidity"])
    t_col = find_col(rhdf, ["tempRHMean", "airTempMean"], contains=["temprhmean", "airtemp", "ta"])
    qf_rh = qf_colname(rhdf)
    rh_use, z_rh = collapse_by_time(
        rhdf, t_rh, [rh_col, t_col], qf=qf_rh, vertical_select=select_height
    )
    rh_use = rh_use.rename(columns={t_rh: "time", rh_col: "RH_pct", t_col: "T_C"})
    meta["rh_qf"] = qf_rh
    meta["rh_height_m"] = z_rh

    # ---- Wind speed ----
    wind = read_table_anycase(tables_dir / stem_wind)
    t_w = find_time_col(wind)
    u_col = find_col(wind, ["windSpeedMean", "windSpeedMean_30min"], contains=["windspeed", "wsmean"])
    qf_w = qf_colname(wind)
    wind_use, z_wind = collapse_by_time(
        wind, t_w, [u_col], qf=qf_w, vertical_select=select_height
    )
    wind_use = wind_use.rename(columns={t_w: "time", u_col: "u_ms"})
    meta["wind_qf"] = qf_w
    meta["wind_height_m"] = z_wind

    # ---- Pressure ----
    pres = read_table_anycase(tables_dir / stem_pres)
    t_p = find_time_col(pres)
    # NEON barometric pressure (DP1.00004.001) commonly uses `staPresMean` (station pressure mean)
    # and/or `corPres` (corrected pressure). Units may be kPa, hPa, or Pa depending on processing.
    p_col = find_col(
        pres,
        ["staPresMean", "corPres", "pressureMean", "atmosPressMean", "presMean", "barPresMean"],
        contains=["stapres", "corpres", "pressure", "pres"],
    )
    qf_p = qf_colname(pres)
    pres_use, _ = collapse_by_time(pres, t_p, [p_col], qf=qf_p, vertical_select=None)
    pres_use = pres_use.rename(columns={t_p: "time", p_col: "p_raw"})
    meta["pres_qf"] = qf_p
    meta["pres_col"] = p_col

    # ---- Merge everything on time ----
    df = flux_use.merge(rad_use, on="time", how="inner")
    df = df.merge(g_use, on="time", how="inner")
    df = df.merge(rh_use, on="time", how="inner")
    df = df.merge(wind_use, on="time", how="inner")
    df = df.merge(pres_use, on="time", how="inner")

    # enforce uniform dt by sorting and dropping duplicates
    df = df.sort_values("time").drop_duplicates("time").reset_index(drop=True)

    # quick dt check (informational; don't hard-fail)
    if len(df) > 3:
        dt_med = df["time"].diff().dt.total_seconds().median()
        meta["dt_median_seconds"] = float(dt_med) if pd.notnull(dt_med) else None
    meta["dt_minutes_arg"] = dt_minutes

    return df, meta


def compute_derived_fluxes_30min(
    df: pd.DataFrame,
    *,
    canopy_height_m: float,
    z_ref_m: Optional[float],
    alpha_pt: float,
    ra_mode: str,
    ra_const_s_m: float,
    clip_negative: bool,
    ignore_g: bool,
) -> pd.DataFrame:
    """
    Given merged 30-min dataframe, compute:
      - Rn, RnG
      - LE_pa (rs=0)
      - LE_p0_eq (α=1)
      - LE_p0_PT (α=alpha_pt)
    """
    d = df.copy()

    if ignore_g:
        d["G_Wm2"] = 0.0

    # Net radiation
    d["Rn_Wm2"] = (d["inSW"] - d["outSW"]) + (d["inLW"] - d["outLW"])
    d["RnG_Wm2"] = d["Rn_Wm2"] - d["G_Wm2"]

    # Pressure to kPa
    d["p_kPa"] = pressure_to_kpa(d["p_raw"].to_numpy())
    d["RH_frac"] = d["RH_pct"] / 100.0

    # Aerodynamic resistance
    if ra_mode.lower() == "constant":
        d["r_a_s_m"] = float(ra_const_s_m)
        used_z = float(z_ref_m) if z_ref_m is not None else np.nan
    else:
        # choose z_ref_m if provided; else expect user selected a wind height and it was stored in meta
        if z_ref_m is None:
            # if not provided, use a common eddy-cov / tower height scale (20 m). User should override.
            used_z = 20.0
        else:
            used_z = float(z_ref_m)
        d["r_a_s_m"] = ra_neutral_s_per_m(
            d["u_ms"].to_numpy(),
            z_m=used_z,
            h_canopy_m=float(canopy_height_m),
        )
    d.attrs["z_ref_m_used"] = used_z

    # Compute PM saturated-patch LE_pa
    d["LE_pa_Wm2"] = penman_monteith_LE_Wm2(
        d["RnG_Wm2"].to_numpy(),
        d["T_C"].to_numpy(),
        d["RH_frac"].to_numpy(),
        d["p_kPa"].to_numpy(),
        d["u_ms"].to_numpy(),
        d["r_a_s_m"].to_numpy(),
        r_s_s_m=0.0,
        clip_negative=clip_negative,
    )

    # Priestley–Taylor wet benchmarks from same air state
    d["LE_p0_eq_Wm2"] = priestley_taylor_LE_Wm2(
        d["RnG_Wm2"].to_numpy(),
        d["T_C"].to_numpy(),
        d["p_kPa"].to_numpy(),
        alpha=1.0,
        clip_negative=clip_negative,
    )
    d["LE_p0_PT_Wm2"] = priestley_taylor_LE_Wm2(
        d["RnG_Wm2"].to_numpy(),
        d["T_C"].to_numpy(),
        d["p_kPa"].to_numpy(),
        alpha=float(alpha_pt),
        clip_negative=clip_negative,
    )

    # Optionally clip observed LE (condensation at night)
    if clip_negative:
        d["LE_obs_Wm2"] = np.maximum(0.0, d["LE_obs_Wm2"].to_numpy())

    return d


def aggregate_to_daily(
    df30: pd.DataFrame,
    *,
    dt_minutes: int,
    min_coverage: float,
) -> pd.DataFrame:
    """
    Aggregate 30-min fluxes to daily totals (MJ/m2/day) and compute nondimensional x,y.

    IMPORTANT: We compute totals by integrating W/m2 over time. Ratios x,y are computed
    on these totals to ensure consistent time coverage.
    """
    d = df30.copy()
    d["date"] = d["time"].dt.floor("D")

    dt_s = float(dt_minutes) * 60.0
    # integrate energy to MJ/m2 for each interval
    for col in ["LE_obs_Wm2", "LE_pa_Wm2", "LE_p0_eq_Wm2", "LE_p0_PT_Wm2", "Rn_Wm2", "RnG_Wm2"]:
        d[f"{col}_MJ_int"] = d[col].to_numpy() * dt_s / 1e6

    agg = d.groupby("date").agg(
        n_int=("time", "count"),
        LE_obs_MJ_d=("LE_obs_Wm2_MJ_int", "sum"),
        LE_pa_MJ_d=("LE_pa_Wm2_MJ_int", "sum"),
        LE_p0_eq_MJ_d=("LE_p0_eq_Wm2_MJ_int", "sum"),
        LE_p0_PT_MJ_d=("LE_p0_PT_Wm2_MJ_int", "sum"),
        Rn_MJ_d=("Rn_Wm2_MJ_int", "sum"),
        RnG_MJ_d=("RnG_Wm2_MJ_int", "sum"),
        T_C=("T_C", "mean"),
        RH_pct=("RH_pct", "mean"),
        u_ms=("u_ms", "mean"),
        p_kPa=("p_kPa", "mean"),
        r_a_s_m=("r_a_s_m", "mean"),
    ).reset_index()

    expected = expected_intervals_per_day(dt_minutes)
    agg["expected_int"] = expected
    agg["coverage"] = agg["n_int"] / expected

    # compute ratios on totals (MJ/m2/day)
    agg["y"] = agg["LE_obs_MJ_d"] / agg["LE_pa_MJ_d"]
    agg["x_eq"] = agg["LE_p0_eq_MJ_d"] / agg["LE_pa_MJ_d"]
    agg["x_PT"] = agg["LE_p0_PT_MJ_d"] / agg["LE_pa_MJ_d"]

    # keep also mm/day for intuition (use lambda at daily-mean T)
    lam = latent_heat_vaporization_J_per_kg(agg["T_C"].to_numpy()) / 1e6  # MJ/kg == MJ/mm
    agg["E_mm_d"] = agg["LE_obs_MJ_d"] / lam
    agg["Epa_mm_d"] = agg["LE_pa_MJ_d"] / lam
    agg["Ep0eq_mm_d"] = agg["LE_p0_eq_MJ_d"] / lam
    agg["Ep0PT_mm_d"] = agg["LE_p0_PT_MJ_d"] / lam

    # Recommended day filter flag (do not drop here; let plotting script decide)
    agg["good_day"] = (agg["coverage"] >= float(min_coverage)) & np.isfinite(agg["y"]) & np.isfinite(agg["x_eq"])

    return agg


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    ap = argparse.ArgumentParser(
        description="Compute daily CR variables from NEON 30-min tables (PM-consistent Epa)."
    )
    ap.add_argument("--root", type=str, default="data/neon", help="Root directory containing site folders.")
    ap.add_argument("--site", type=str, required=True, help="4-letter NEON site code, e.g. HARV.")
    ap.add_argument("--start", type=str, required=True, help="YYYY-MM start (inclusive), e.g. 2019-06.")
    ap.add_argument("--end", type=str, required=True, help="YYYY-MM end (inclusive), e.g. 2019-09.")
    ap.add_argument("--dt-minutes", type=int, default=30, help="Native timestep minutes (default 30).")

    # table stems
    ap.add_argument("--stem-flux", type=str, default="eddy_fluxH2O_dp04_30min")
    ap.add_argument("--stem-rad", type=str, default="rad_DP1_00023_001_30min")
    ap.add_argument("--stem-g", type=str, default="g_DP1_00040_001_30min")
    ap.add_argument("--stem-rh", type=str, default="rh_DP1_00098_001_30min")
    ap.add_argument("--stem-wind", type=str, default="wind_DP1_00001_001_30min")
    ap.add_argument("--stem-pres", type=str, default="pres_DP1_00004_001_30min")

    # physics / filtering
    ap.add_argument("--alpha-pt", type=float, default=1.26, help="Priestley–Taylor alpha (default 1.26).")
    ap.add_argument("--min-coverage", type=float, default=0.8, help="Min daily timestep coverage to flag good_day.")
    ap.add_argument(
        "--clip-negative",
        action="store_true",
        help="Clip negative RnG and negative LE to 0 before aggregating (recommended for daily ET).",
    )
    ap.add_argument(
        "--ignore-g",
        action="store_true",
        help="Ignore soil heat flux by setting G=0 when computing RnG (often acceptable at daily scale).",
    )


    # heights / aerodynamic resistance
    ap.add_argument(
        "--select-height",
        type=str,
        default="max",
        help="When DP1 tables have multiple verticalPosition values, select 'max', 'min', or a number.",
    )
    ap.add_argument(
        "--canopy-height-m",
        type=float,
        default=20.0,
        help="Canopy height (m) for neutral log-law ra estimate (default 20 m; adjust per site).",
    )
    ap.add_argument(
        "--z-ref-m",
        type=float,
        default=None,
        help="Reference height (m) for ra calculation. If omitted and ra-mode=loglaw, defaults to 20 m.",
    )
    ap.add_argument(
        "--ra-mode",
        type=str,
        choices=["loglaw", "constant"],
        default="loglaw",
        help="Aerodynamic resistance model: neutral log-law (default) or constant.",
    )
    ap.add_argument(
        "--ra-const",
        type=float,
        default=50.0,
        help="Constant aerodynamic resistance (s/m) if ra-mode=constant.",
    )

    ap.add_argument("--outdir", type=str, default=None, help="Output directory (defaults to site folder).")
    return ap.parse_args(argv)


def main(argv: Optional[Sequence[str]] = None) -> int:
    args = parse_args(argv)

    site_tag = f"{args.site}_{args.start}_{args.end}"
    root = Path(args.root)
    base = root / site_tag
    tables_dir = base / "tables"
    if args.outdir is None:
        outdir = base
    else:
        outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    if not tables_dir.exists():
        raise FileNotFoundError(f"tables/ not found at: {tables_dir}")

    df30, meta = build_30min_dataframe(
        tables_dir,
        stem_flux=args.stem_flux,
        stem_rad=args.stem_rad,
        stem_g=args.stem_g,
        stem_rh=args.stem_rh,
        stem_wind=args.stem_wind,
        stem_pres=args.stem_pres,
        dt_minutes=int(args.dt_minutes),
        select_height=args.select_height,
    )

    df30 = compute_derived_fluxes_30min(
        df30,
        canopy_height_m=float(args.canopy_height_m),
        z_ref_m=args.z_ref_m,
        alpha_pt=float(args.alpha_pt),
        ra_mode=args.ra_mode,
        ra_const_s_m=float(args.ra_const),
        clip_negative=bool(args.clip_negative),
        ignore_g=bool(args.ignore_g),
    )

    daily = aggregate_to_daily(df30, dt_minutes=int(args.dt_minutes), min_coverage=float(args.min_coverage))

    out_csv = outdir / "daily_cr_demo.csv"
    daily.to_csv(out_csv, index=False)

    # QC summary
    good = daily[daily["good_day"]].copy()
    frac_y_gt1 = float(np.mean(good["y"] > 1.0)) if len(good) else float("nan")
    print("\n--- NEON daily CR demo: QC summary ---")
    print(f"Site window: {site_tag}")
    print(f"Rows (30-min merged): {len(df30)}")
    print(f"Days total: {len(daily)} ; good_day (coverage>={args.min_coverage}): {len(good)}")
    print(f"Median y=E/Epa (good days): {np.nanmedian(good['y']):.3f}")
    print(f"Fraction of good days with y>1: {frac_y_gt1:.3f}")
    print(f"Median x_eq=Ep0eq/Epa: {np.nanmedian(good['x_eq']):.3f}")
    print(f"Median x_PT=Ep0PT/Epa: {np.nanmedian(good['x_PT']):.3f}")
    print(f"Output CSV: {out_csv}")

    # Helpful meta
    if meta.get("wind_height_m", None) is not None:
        print(f"Wind selected height (verticalPosition): {meta['wind_height_m']} m")
    if meta.get("rh_height_m", None) is not None:
        print(f"RH/T selected height (verticalPosition): {meta['rh_height_m']} m")
    z_used = df30.attrs.get("z_ref_m_used", None)
    if z_used is not None:
        print(f"ra reference height used (z_ref_m): {z_used}")

    return 0


if __name__ == "__main__":
    sys.exit(main())